{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, operator, nltk, csv\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = 'Dataset/500N-KPCrowd-v1.1/CorpusAndCrowdsourcingAnnotations/'\n",
    "    \n",
    "for subdir, dirs, files in os.walk(dataset_dir):\n",
    "    # iterate over all files in train/test directory\n",
    "    for f in files:\n",
    "        if not f.endswith('.txt') or f.endswith('justTitle.txt'):\n",
    "            continue\n",
    "        fopen = open(subdir+'/'+f, 'r')#, encoding=\"utf-8\")\n",
    "        text = fopen.read().decode('utf-8')\n",
    "        # tokenizing the text and filtering nouns / verbs etc\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens_non_stop = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "        def filter_words(key):\n",
    "            return True if key.startswith('NN') or key.startswith('J') or key.startswith('VB') else False\n",
    "        filt_text = map(lambda key: key[0], filter(lambda key: filter_words(key[1]), pos_tag(tokens_non_stop)))\n",
    "        \n",
    "        # creating local maps for nodes and edges\n",
    "        dict_text = defaultdict(int)\n",
    "        for t in filt_text:\n",
    "            dict_text[t] += 1\n",
    "        edges, nodes, nodes_number = [], {}, {}\n",
    "        c = 0\n",
    "        for d in dict_text:\n",
    "            if d in nodes:\n",
    "                continue\n",
    "            nodes[d] = c\n",
    "            nodes_number[c] = d\n",
    "            c += 1\n",
    "            \n",
    "        # create edges for graph-of-words\n",
    "        k = min(10, len(filt_text)-1) ## window size\n",
    "        for i in range(len(filt_text)-k):\n",
    "            for j in range(i+1, i+k+1):\n",
    "                wt = j-i  ## this is the weight of the edge (distance between words)\n",
    "                edges.append((nodes[filt_text[i]], nodes[filt_text[j]], wt))\n",
    "\n",
    "        newDir = '/'.join(subdir.split('/')[:-2])\n",
    "        newFile = newDir + '/graphEdges/' + subdir.split('/')[-1] + '/' + f\n",
    "        newMapFile = newFile[:-4]+'-map.txt'\n",
    "        with open(newFile, \"w+\") as wrt:\n",
    "            out = csv.writer(wrt)\n",
    "            out.writerow(('node1', 'node2', 'weight'))\n",
    "            for e in edges:\n",
    "                out.writerow(e)\n",
    "        with open(newMapFile, \"w+\") as wrt:\n",
    "            out = csv.writer(wrt)\n",
    "            out.writerow(('word', 'node'))\n",
    "            for n in nodes:\n",
    "                out.writerow((n.encode('utf-8'), nodes[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_graph_dir = 'Dataset/500N-KPCrowd-v1.1/graphEdges/'\n",
    "    \n",
    "for subdir, dirs, files in os.walk(dataset_graph_dir):\n",
    "    # iterate over all files in train/test directory\n",
    "    for f in files:\n",
    "        if f.endswith('-map.txt'):\n",
    "            continue\n",
    "        \n",
    "        edges = []\n",
    "        nodeToWordMap, wordToNodeMap = {}, {}\n",
    "       \n",
    "        # reading map of nodes to words\n",
    "        with open(subdir + '/' + f[:-4] + '-map.txt', \"r\") as fopen:\n",
    "            reader = csv.reader(fopen)\n",
    "            graph_nodes_map  = list(reader)[1:]\n",
    "        for g in graph_nodes_map:\n",
    "            nodeToWordMap[int(g[1])] = g[0]\n",
    "            wordToNodeMap[g[0]] = int(g[1])\n",
    "\n",
    "        # reading edge list for the graph\n",
    "        with open(subdir + '/' + f, \"r\") as fopen:\n",
    "            reader = csv.reader(fopen)\n",
    "            graph_edges  = list(reader)[1:]\n",
    "        for g in graph_edges:\n",
    "            edges.append((int(g[0]), int(g[1]), int(g[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Dataset/500N-KPCrowd-v1.1/CorpusAndCrowdsourcingAnnotations/train/art_and_culture-20893614.txt', 'r')\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the text and filtering nouns / verbs etc\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens_non_stop = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "def filter_words(key):\n",
    "    return True if key.startswith('NN') or key.startswith('J') or key.startswith('VB') else False\n",
    "filt_text = map(lambda key: key[0], filter(lambda key: filter_words(key[1]), pos_tag(tokens_non_stop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating local maps for nodes and edges\n",
    "dict_text = defaultdict(int)\n",
    "for t in filt_text:\n",
    "    dict_text[t] += 1\n",
    "edges = []\n",
    "nodes = {}\n",
    "nodes_number = {}\n",
    "c = 0\n",
    "for d in dict_text:\n",
    "    if d in nodes:\n",
    "        continue\n",
    "    nodes[d] = c\n",
    "    nodes_number[c] = d\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create edges for graph-of-words\n",
    "k = 5 ## window size\n",
    "for i in range(len(filt_text)-k):\n",
    "    for j in range(i+1, i+k+1):\n",
    "        wt = j-i  ## this is the weight of the edge (distance between words)\n",
    "        edges.append((nodes[filt_text[i]], nodes[filt_text[j]], wt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file and store the edges\n",
    "#fw = open('Dataset/500N-KPCrowd-Edges/train/art_and_culture-20893614.txt', 'w')\n",
    "#fw.write(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create graph-of-words\n",
    "G=nx.Graph()\n",
    "\n",
    "### TODO: read from file to add edges\n",
    "G.add_weighted_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lopilato\n",
      "Buble\n",
      "Aires\n",
      "Buenos\n",
      "Argentine\n",
      "fans\n",
      "Michael\n",
      "pop\n",
      "ceremony\n",
      "civil\n",
      "actress\n",
      "month\n",
      "Luisana\n",
      "Thursday\n",
      "wedding\n",
      "concert\n",
      "star\n",
      "TV\n",
      "Argentina\n",
      "wore\n",
      "Canadian\n",
      "AP\n",
      "including\n",
      "mansion\n",
      "top-grossing\n",
      "March\n",
      "rose\n",
      "young\n",
      "bouquet\n",
      "rice\n"
     ]
    }
   ],
   "source": [
    "# extracting keywords using simple degree centrality\n",
    "centrality = nx.degree_centrality(G)\n",
    "sorted_centr = sorted(centrality, key=centrality.get, reverse=True)\n",
    "for i in sorted_centr[:30]:\n",
    "    print nodes_number[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lopilato\n",
      "Buble\n",
      "Buenos\n",
      "Aires\n",
      "fans\n",
      "Argentine\n",
      "pop\n",
      "Michael\n",
      "ceremony\n",
      "civil\n",
      "actress\n",
      "month\n",
      "Luisana\n",
      "concert\n",
      "star\n",
      "Thursday\n",
      "wedding\n",
      "TV\n",
      "wore\n",
      "day\n",
      "Canadian\n",
      "messages\n",
      "Argentina\n",
      "AP\n",
      "sent\n",
      "Twitter\n",
      "Facebook\n",
      "followers\n",
      "Juan\n",
      "Little\n"
     ]
    }
   ],
   "source": [
    "# extracting keywords using page-rank\n",
    "page_ranks = nx.pagerank(G)\n",
    "sorted_page_ranks = sorted(page_ranks, key=page_ranks.get, reverse=True)\n",
    "for i in sorted_page_ranks[:30]:\n",
    "    print nodes_number[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
