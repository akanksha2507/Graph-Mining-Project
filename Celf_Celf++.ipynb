{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, operator, nltk\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('Dataset/500N-KPCrowd-v1.1/CorpusAndCrowdsourcingAnnotations/train/art_and_culture-20893614.txt', 'r')\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizing the text and filtering nouns / verbs etc\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens_non_stop = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "def filter_words(key):\n",
    "    return True if key.startswith('NN') or key.startswith('JJ') or key.startswith('VB') else False\n",
    "filt_text = map(lambda key: key[0], filter(lambda key: filter_words(key[1]), pos_tag(tokens_non_stop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating local maps for nodes and edges\n",
    "dict_text = defaultdict(int)\n",
    "for t in filt_text:\n",
    "    dict_text[t] += 1\n",
    "edges = []\n",
    "nodes = {}\n",
    "nodes_number = {}\n",
    "c = 0\n",
    "for d in dict_text:\n",
    "    if d in nodes:\n",
    "        continue\n",
    "    nodes[d] = c\n",
    "    nodes_number[c] = d\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create edges for graph-of-words\n",
    "edges1=[]\n",
    "k = 5 ## window size\n",
    "for i in range(len(filt_text)-k):\n",
    "    for j in range(i+1, i+k+1):\n",
    "        wt = j-i  ## this is the weight of the edge (distance between words)\n",
    "        edges.append((nodes[filt_text[i]], nodes[filt_text[j]], 1.0/wt))\n",
    "        edges1.append((nodes[filt_text[i]], nodes[filt_text[j]]))\n",
    "        edges1.append((nodes[filt_text[j]], nodes[filt_text[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create graph-of-words\n",
    "G=nx.Graph()\n",
    "\n",
    "### TODO: read from file to add edges\n",
    "G.add_edges_from(edges1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degree={}\n",
    "for node in G:\n",
    "    degree[node]=G.degree(node)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fw = open('Dataset/edges_celf.txt', 'w')\n",
    "for line in edges1:\n",
    "    fw.write(\"%s\\n\" % (str(line[0])+\" \"+str(line[1])+\" \"+str(1.0/degree[line[1]])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output={}\n",
    "file = open('Dataset/IC_CelfPlus_Greedy.txt', 'r')\n",
    "for line in file:\n",
    "    line=line.split()\n",
    "    output[line[0]]=line[2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_output = sorted(output.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "keywords = []\n",
    "for item in sorted_output:\n",
    "    keywords.append(nodes_number[int(item[0])])\n",
    "#     print nodes_number[int(item[0])]\n",
    "print len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeMetric(origKeys, predKeys):\n",
    "    n_hits = 0\n",
    "    n_misses = 0\n",
    "    n_false_alarms = 0\n",
    "    for k in predKeys:\n",
    "        if k in origKeys:\n",
    "            n_hits += 1\n",
    "        else:\n",
    "            n_false_alarms += 1\n",
    "    for k in origKeys:\n",
    "        if k not in predKeys:\n",
    "            n_misses += 1\n",
    "  \n",
    "    precision = n_hits*1.0/(n_hits+n_false_alarms)\n",
    "    recall = n_hits*1.0/(n_hits+n_misses)\n",
    "  \n",
    "    F1_score = 2.0*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print (\"Precision = \" + str(precision))\n",
    "    print (\"Recall = \" + str(recall))\n",
    "    print (\"F1 score = \" + str(F1_score))\n",
    "    #return(list(precision, recall, F1_score))    \n",
    "\n",
    "def combinePhrases(initKeys, split_text):\n",
    "    result = []\n",
    "    len_text = len(split_text)\n",
    "    for i in xrange(len_text):\n",
    "        word = split_text[i]\n",
    "        if word in initKeys:\n",
    "            combined_word = [word]\n",
    "            if i + 1 == len_text: result.append(word)   # appends last word if keyword and doesn't iterate\n",
    "            for j in xrange(i + 1, len_text):\n",
    "                other_word = split_text[j]\n",
    "                if other_word in initKeys:\n",
    "                    combined_word.append(other_word)\n",
    "                else:\n",
    "                    for w in combined_word: \n",
    "                        if w in initKeys:\n",
    "                            initKeys.remove(w)\n",
    "                    result.append(\" \".join(combined_word))\n",
    "                    break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('Dataset/500N-KPCrowd-v1.1/CorpusAndCrowdsourcingAnnotations/train/art_and_culture-20893614.key', 'r')\n",
    "origKeys = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predCelfKeys = combinePhrases(keywords, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['married', 'Argentine', 'his', 'Grammy', 'ceremony', 'kisses', 'messages', 'romantically', 'vocal', 'Argentine', 'woman', 'Buble', 'dated', 'thanking', 'wedding', 'ceremony', 'Canadian', 'Michael', 'bouquet', 'vocal', 'album', 'Crazy', 'Love', 'Canadian', 'pop', 'Argentina', 'AP', 'mansion', 'Facebook', 'rose', 'Argentine', 'sweetheart', 'month', 'registry', 'South', 'American', 'after', 'tying', 'downtown', 'beautiful', 'traditional', 'Grammy', 'Vancouver', 'chiffon', 'dress', 'Grammywinning', 'family', 'Lopilato', 'made', 'North', 'bride', 'Argentine', 'TV', 'actress', 'media', 'Twitter', 'met', 'high']\n",
      "['Buble', 'month', 'Aires', 'BUENOS', 'Way', 'bouquet', 'Soul', 'woman', 'Thursday', 'civil', 'Lopilato', 'shouted', 'heels', 'South', 'Photo/Natacha', 'Buenos', 'Angeles', 'actress', 'Grammy-winning', 'several', 'ceremony', 'North', 'Los', 'Rebel', 'mansion', 'Canadian', 'kisses', 'Argentine', 'Michael', 'operas', 'TV', 'years', 'pop', 'guests', 'met', 'star', 'Vancouver', 'sweetheart', 'involved', 'couple', 'last', 'chiffon', 'orchids', 'traditional', 'full', 'March', 'purple', 'bride', 'Children', 'Luisana']\n"
     ]
    }
   ],
   "source": [
    "# For CELF\n",
    "orig_keys = []\n",
    "for origkey in origKeys:\n",
    "    orig_keys.extend(origkey.split())\n",
    "print orig_keys\n",
    "print keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.46\n",
      "Recall = 0.433962264151\n",
      "F1 score = 0.446601941748\n"
     ]
    }
   ],
   "source": [
    "keywords_new=\" \".join(keywords)\n",
    "keywords_new=keywords_new.translate(None, string.punctuation)\n",
    "computeMetric(orig_keys,keywords_new.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
