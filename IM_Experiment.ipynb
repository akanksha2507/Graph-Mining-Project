{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, operator, nltk\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "import networkx as nx\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = 'Dataset/500N-KPCrowd-v1.1/CorpusAndCrowdsourcingAnnotations/'\n",
    "    \n",
    "for subdir, dirs, files in os.walk(dataset_dir):\n",
    "    # iterate over all files in train/test directory\n",
    "    for file in files:\n",
    "        if not file.endswith('.txt'):\n",
    "            continue\n",
    "#         f = open(subdir+'/'+file, 'r', encoding='utf-8')\n",
    "        f = codecs.open(subdir+'/'+file, \"r\", \"utf-8\" )\n",
    "        text = f.read()\n",
    "#         print file\n",
    "        # tokenizing the text and filtering nouns / verbs etc\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens_non_stop = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "        def filter_words(key):\n",
    "            return True if key.startswith('NN') or key.startswith('J') or key.startswith('VB') else False\n",
    "        filt_text = map(lambda key: key[0], filter(lambda key: filter_words(key[1]), pos_tag(tokens_non_stop)))\n",
    "        \n",
    "        # creating local maps for nodes and edges\n",
    "        dict_text = defaultdict(int)\n",
    "        for t in filt_text:\n",
    "            dict_text[t] += 1\n",
    "        edges, nodes, nodes_number = [], {}, {}\n",
    "        c = 0\n",
    "        for d in dict_text:\n",
    "            if d in nodes:\n",
    "                continue\n",
    "            nodes[d] = c\n",
    "            nodes_number[c] = d\n",
    "            c += 1\n",
    "            \n",
    "        # create edges for graph-of-words\n",
    "        k = 10 ## window size\n",
    "        for i in range(len(filt_text)-k):\n",
    "            for j in range(i+1, i+k+1):\n",
    "                wt = j-i  ## this is the weight of the edge (distance between words)\n",
    "                edges.append((nodes[filt_text[i]], nodes[filt_text[j]], wt))\n",
    "print \"Done!\"\n",
    "        # TODO XX_Jahnavi: Write these edges to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f = open('Dataset/500N-KPCrowd-v1.1/CorpusAndCrowdsourcingAnnotations/train/art_and_culture-20893614.txt', 'r')\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tokenizing the text and filtering nouns / verbs etc\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens_non_stop = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "def filter_words(key):\n",
    "    return True if key.startswith('NN') or key.startswith('J') or key.startswith('VB') else False\n",
    "filt_text = map(lambda key: key[0], filter(lambda key: filter_words(key[1]), pos_tag(tokens_non_stop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# creating local maps for nodes and edges\n",
    "dict_text = defaultdict(int)\n",
    "for t in filt_text:\n",
    "    dict_text[t] += 1\n",
    "edges = []\n",
    "nodes = {}\n",
    "nodes_number = {}\n",
    "c = 0\n",
    "for d in dict_text:\n",
    "    if d in nodes:\n",
    "        continue\n",
    "    nodes[d] = c\n",
    "    nodes_number[c] = d\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create edges for graph-of-words\n",
    "k = 5 ## window size\n",
    "for i in range(len(filt_text)-k):\n",
    "    for j in range(i+1, i+k+1):\n",
    "        wt = j-i  ## this is the weight of the edge (distance between words)\n",
    "        edges.append((nodes[filt_text[i]], nodes[filt_text[j]], wt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create graph-of-words\n",
    "G=nx.Graph()\n",
    "\n",
    "### TODO: read from file to add edges\n",
    "G.add_weighted_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priority Queue implementation used by IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from heapq import *\n",
    "\n",
    "class PriorityQueue(object):\n",
    "    def __init__(self):\n",
    "        self.pq = []                         # list of entries arranged in a heap\n",
    "        self.entry_finder = {}               # mapping of tasks to entries\n",
    "        self.REMOVED = '<removed-task>'      # placeholder for a removed task\n",
    "        self.counter = itertools.count()     # unique sequence count\n",
    "\n",
    "    def add_task(self, task, priority=0):\n",
    "        'Add a new task or update the priority of an existing task'\n",
    "        if task in self.entry_finder:\n",
    "            self.remove_task(task)\n",
    "        count = next(self.counter)\n",
    "        entry = [priority, count, task]\n",
    "        self.entry_finder[task] = entry\n",
    "        heappush(self.pq, entry)\n",
    "\n",
    "    def remove_task(self, task):\n",
    "        'Mark an existing task as REMOVED.  Raise KeyError if not found.'\n",
    "        entry = self.entry_finder.pop(task)\n",
    "        entry[-1] = self.REMOVED\n",
    "\n",
    "    def pop_item(self):\n",
    "        'Remove and return the lowest priority task. Raise KeyError if empty.'\n",
    "        while self.pq:\n",
    "            priority, count, task = heappop(self.pq)\n",
    "            if task is not self.REMOVED:\n",
    "                del self.entry_finder[task]\n",
    "                return task, priority\n",
    "        raise KeyError('pop from an empty priority queue')\n",
    "\n",
    "    def __str__(self):\n",
    "        return str([entry for entry in self.pq if entry[2] != self.REMOVED])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IC function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runIC (G, S, p = .01):\n",
    "    ''' Runs independent cascade model.\n",
    "    Input: G -- networkx graph object\n",
    "    S -- initial set of vertices\n",
    "    p -- propagation probability\n",
    "    Output: T -- resulted influenced set of vertices (including S)\n",
    "    '''\n",
    "    from copy import deepcopy\n",
    "    from random import random\n",
    "    T = deepcopy(S) # copy already selected nodes\n",
    "\n",
    "    # ugly C++ version\n",
    "    i = 0\n",
    "    while i < len(T):\n",
    "        for v in G[T[i]]: # for neighbors of a selected node\n",
    "            if v not in T: # if it wasn't selected yet\n",
    "                w = G[T[i]][v]['weight'] # count the number of edges between two nodes\n",
    "                if random() <= 1 - (1-p)**w: # if at least one of edges propagate influence\n",
    "                    # print T[i], 'influences', v\n",
    "                    T.append(v)\n",
    "        i += 1\n",
    "\n",
    "    # neat pythonic version\n",
    "    # legitimate version with dynamically changing list: http://stackoverflow.com/a/15725492/2069858\n",
    "    # for u in T: # T may increase size during iterations\n",
    "    #     for v in G[u]: # check whether new node v is influenced by chosen node u\n",
    "    #         w = G[u][v]['weight']\n",
    "    #         if v not in T and random() < 1 - (1-p)**w:\n",
    "    #             T.append(v)\n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Greedy algo from Kempe paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from priorityQueue import PriorityQueue as PQ\n",
    "# from IC.IC import runIC\n",
    "\n",
    "def generalGreedy(G, k, p=.01):\n",
    "    ''' Finds initial seed set S using general greedy heuristic\n",
    "    Input: G -- networkx Graph object\n",
    "    k -- number of initial nodes needed\n",
    "    p -- propagation probability\n",
    "    Output: S -- initial set of k nodes to propagate\n",
    "    '''\n",
    "    import time\n",
    "    start = time.time()\n",
    "    R = 5 # number of times to run Random Cascade\n",
    "    S = [] # set of selected nodes\n",
    "    # add node to S if achieves maximum propagation for current chosen + this node\n",
    "    for i in range(k):\n",
    "        print i, \" \",\n",
    "        s = PQ() # priority queue\n",
    "        for v in G.nodes():\n",
    "            if v not in S:\n",
    "                s.add_task(v, 0) # initialize spread value\n",
    "                for j in range(R): # run R times Random Cascade\n",
    "                    [priority, count, task] = s.entry_finder[v]\n",
    "                    s.add_task(v, priority - float(len(runIC(G, S + [v], p)))/R) # add normalized spread value\n",
    "        task, priority = s.pop_item()\n",
    "        S.append(task)\n",
    "#         print i, k, time.time() - start\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   1   2   3   4   5   6   7   8   9   10   11   12   13   14   15   16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33  "
     ]
    }
   ],
   "source": [
    "result = generalGreedy(G, 58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sitcoms\n",
      "Lopilato\n",
      "sweetheart\n",
      "traditional\n",
      "civil\n",
      "Facebook\n",
      "homes\n",
      "day\n",
      "threw\n",
      "album\n",
      "followers\n",
      "years\n",
      "Grammy\n",
      "dress\n",
      "Buenos\n",
      "woman\n",
      "guests\n",
      "Canada\n",
      "gray\n",
      "star\n",
      "Children\n",
      "top-grossing\n",
      "Monaco\n",
      "British\n",
      "love\n",
      "Girls\n",
      "including\n",
      "BUENOS\n",
      "high\n",
      "wedding\n",
      "pop\n",
      "model\n",
      "several\n",
      "sharp\n",
      "Michael\n",
      "Soul\n",
      "young\n",
      "TV\n",
      "media\n",
      "AIRES\n",
      "shouted\n",
      "American\n",
      "Pisarenko\n",
      "suit\n",
      "orchids\n",
      "tennis\n",
      "tour\n",
      "petals\n",
      "player\n",
      "Los\n",
      "ceremony\n",
      "full\n",
      "Pirate\n",
      "Blunt\n",
      "rose\n",
      "wore\n",
      "Crazy\n",
      "red\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "for node_num in result:\n",
    "    prediction.append(nodes_number[node_num])\n",
    "    print nodes_number[node_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def computeMetric(origKeys, predKeys):\n",
    "    print origKeys\n",
    "    print predKeys\n",
    "    n_hits = 0\n",
    "    n_misses = 0\n",
    "    n_false_alarms = 0\n",
    "    for k in predKeys:\n",
    "        if k in origKeys:\n",
    "            n_hits += 1\n",
    "        else:\n",
    "            n_false_alarms += 1\n",
    "    for k in origKeys:\n",
    "        if k not in predKeys:\n",
    "            n_misses += 1\n",
    "  \n",
    "    precision = n_hits*1.0/(n_hits+n_false_alarms)\n",
    "    recall = n_hits*1.0/(n_hits+n_misses)\n",
    "  \n",
    "    F1_score = 2.0*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print (\"Precision = \" + str(precision))\n",
    "    print (\"Recall = \" + str(recall))\n",
    "    print (\"F1 score = \" + str(F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['married', 'Argentine', 'his', 'Grammy', 'ceremony', 'kisses', 'messages', 'romantically', 'vocal', 'Argentine', 'woman', 'Buble', 'dated', 'thanking', 'wedding', 'ceremony', 'Canadian', 'Michael', 'bouquet', 'vocal', 'album', 'Crazy', 'Love', 'Canadian', 'pop', 'Argentina', 'AP', 'mansion', 'Facebook', 'rose', 'Argentine', 'sweetheart', 'month', 'registry', 'South', 'American', 'after', 'tying', 'downtown', 'beautiful', 'traditional', 'Grammy', 'Vancouver', 'chiffon', 'dress', 'Grammywinning', 'family', 'Lopilato', 'made', 'North', 'bride', 'Argentine', 'TV', 'actress', 'media', 'Twitter', 'met', 'high']\n",
      "['sitcoms', 'Lopilato', 'sweetheart', 'traditional', 'civil', 'Facebook', 'homes', 'day', 'threw', 'album', 'followers', 'years', 'Grammy', 'dress', 'Buenos', 'woman', 'guests', 'Canada', 'gray', 'star', 'Children', 'top-grossing', 'Monaco', 'British', 'love', 'Girls', 'including', 'BUENOS', 'high', 'wedding', 'pop', 'model', 'several', 'sharp', 'Michael', 'Soul', 'young', 'TV', 'media', 'AIRES', 'shouted', 'American', 'Pisarenko', 'suit', 'orchids', 'tennis', 'tour', 'petals', 'player', 'Los', 'ceremony', 'full', 'Pirate', 'Blunt', 'rose', 'wore', 'Crazy', 'red']\n",
      "Precision = 0.310344827586\n",
      "Recall = 0.321428571429\n",
      "F1 score = 0.315789473684\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = open('Dataset/500N-KPCrowd-v1.1/CorpusAndCrowdsourcingAnnotations/train/art_and_culture-20893614.key', 'r')\n",
    "origKeys = f.read().split(\"\\n\")\n",
    "keys = []\n",
    "for k in origKeys:\n",
    "    keys.extend(k.split())\n",
    "\n",
    "computeMetric(keys, prediction)\n",
    "# extracting keywords using simple degree centrality\n",
    "# centrality = nx.degree_centrality(G)\n",
    "# sorted_centr = sorted(centrality, key=centrality.get, reverse=True)\n",
    "# predKeys = [nodes_number[i] for i in sorted_centr[:int(0.33*len(sorted_centr))]]\n",
    "# computeMetric(origKeys, predKeys)\n",
    "#for i in sorted_centr[:30]:\n",
    "#    print nodes_number[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
